<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    background-color: white;
    padding: 100px;
    width: 1000px;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Open Sans', sans-serif;
    color: #121212;
  }
  h1, h2, h3, h4 {
    font-family: 'Source Sans Pro', sans-serif;
  }
  kbd {
    color: #121212;
  }
</style>
<title>CS 184 Path Tracer</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    }
  };
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>

</head>


<body>

<h1 align="middle">CS 184: Computer Graphics and Imaging, Spring 2023</h1>
<h1 align="middle">Project 3-1: Path Tracer</h1>
<h2 align="middle">Lawrence Lam</h2>

<!-- Add Website URL -->
<h2 align="middle">Website URL: <a href="https://cal-cs184-student.github.io/hw-webpages-sp24-diandestroyer/hw3/index.html">https://cal-cs184-student.github.io/hw-webpages-sp24-diandestroyer/hw3/index.html</a></h2>

<br><br>
<div>

<h2 align="middle">Overview</h2>
<p>
    In this project I delved into the implementation and optimization of a path tracer for generating realistic images with accurate lighting and shading effects. 
    We stepped through the entire process through converting our pixels in the image space to world space all the way to calculating the radiance of a surface based off of the rays emitted by the light sources and their bounces.
    To step through the entire process we chose a pixel on our image that we want to render, we then convert those pixel coordinates to some ray projected from the camera to the camera sensor. Through this camera ray we are able to record
    the radiance and color of that specific pixel. The radiance and color is determined based on the light sources in the scene where each one emits a number of rays
    and based on rendering options the light rays can bounce, accumulate the bounces or not, change the # of samples per area light, or change the camera sampling rate all which affect the resulting render in different ways.
</p>
<br>

<h2 align="middle">Part 1: Ray Generation and Scene Intersection (20 Points)</h2>
<!-- Walk through the ray generation and primitive intersection parts of the rendering pipeline.
Explain the triangle intersection algorithm you implemented in your own words.
Show images with normal shading for a few small .dae files. -->

<h3>
  Walk through the ray generation and primitive intersection parts of the rendering pipeline.
</h3>
<p>
    For ray generation we start with some coordinates (x,y) in the image space which is normalized on a 2d plane that spans from (0,0) to (1,1). 
    We are trying to maintain this (x,y) position on the original plane as we move into the camera space and generate a ray that starts from the camera and passes through that same pixel in the 3d plane.
    This 3d plane is the camera sensor which spans from (-tan(hFov/2), -tan(vFov/2),-1) to (tan(hFov/2), tan(vFov/2),-1), so the first step we will have to take it to convert our normalized (x,y) coordinates to a pixel on the camera sensor.
    To do this we can scale our (x,y) coordinates by the sensor width and height where width and height are tan(hFov/2) and tan(vFov/2) respectively.
    I used the following equation to scale the x coordinate <p align="middle"><pre align="middle">(1-x) * -sensor_width + x * sensor_width</pre></p> and replaced x with y and width for height for the y coordinate.
    Once this is done we simply multiply the canera-to-world rotation matrix by our newly scaled camera ray vector to transform it into the world space.
</p>
<br>

<h3>
  Explain the triangle and sphere intersection algorithm you implemented in your own words.
</h3>
<p>
    For my triangle intersection algorithm I used the Moller Trumbore Algorithm that was shown in lecture to calculate (t, b1, b2). 
    This entailed calculating the barycentric coordinates using a series of cross products and dot products.
    Using these barycentric coordinates (b1, b2) we are able to determine if our ray is intercepting the plane within the given triangle.
    If the barycentric coordinate values are greater than 0 and less than 1 when added together then the t is a valid intercept if within the bounds r.max_t and r.min_t.
    These barycentric coordinates (1 - b1 - b2, b1, b2) are also used as weights when calulating the normal of the intersection to determine the color of the object. 

    For my sphere intersection I also used the simple intersection algorithm detailed in lecture where you are solving for t using the quadratic formula.
    <p align="middle"><pre align="middle">at^2 + bt + c = 0</pre></p>
    After calculating the 2 t values given the +- in the quadratic formula we determine which of the intercepts is smaller/closer given they are within the bounds r.max_t and r.min_t.
</p>
<br>

<h3>
  Show images with normal shading for a few small .dae files.
</h3>
<h4>**I had a copy paste bug where I was scaling the y coordinate by the sensor_width instead of the sensor_height which I did not catch until Part 3, so renders will look a little squished until then.
  Renders should be correct aside from this scaling issue.
</h4>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/CBemptyp1.png" align="middle" width="400px"/>
        <figcaption>CBempty.dae</figcaption>
      </td>
      <td>
        <img src="images/CBspheresp1.png" align="middle" width="400px"/>
        <figcaption>CBspheres.dae</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/CBcoilp1.png" align="middle" width="400px"/>
        <figcaption>CBcoil.dae</figcaption>
      </td>
      <td>
        <img src="images/bananap1.png" align="middle" width="400px"/>
        <figcaption>banana.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>


<h2 align="middle">Part 2: Bounding Volume Hierarchy (20 Points)</h2>
<!-- Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis. -->

<h3>
  Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
</h3>
<p>
    My BVH construction algorithm first checks if the number of primatives in the given node is less than the max leaf size and if it is simply return a newly made leaf node.
    After that check, I decided on splitting down the bbox axis that was the longest. We can check this using the extent of the bbox and checking which axis is the largest. 
    Once determining the longest axis I create a split point using the mean value of all primatives in the node along that given axis. 
    With a split point it is then an easy task to create 2 new primitive lists for those greater and equal to and those less than the split point value.
    Once the vectors are created simply call construct bch recursively making one side the left and the other the right side of the current node which becomes the parent.
</p>

<h3>
  Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
</h3>
<h4>**I had a copy paste bug where I was scaling the y coordinate by the sensor_width instead of the sensor_height which I did not catch until Part 3, so renders will look a little squished until then.
  Renders should be correct aside from this scaling issue.
</h4>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/CBdragonp2.png" align="middle" width="400px"/>
        <figcaption>CBdragon.dae</figcaption>
      </td>
      <td>
        <img src="images/CBlucyp2.png" align="middle" width="400px"/>
        <figcaption>CBlucy.dae</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/dragonp2.png" align="middle" width="400px"/>
        <figcaption>dragon.dae</figcaption>
      </td>
      <td>
        <img src="images/maxplanckp2.png" align="middle" width="400px"/>
        <figcaption>maxplanck.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3>
  Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis.
</h3>
<p>
    Rendering time for the cow.dae file averages around ~40-50s without BVH implementation but with BVH implementation takes around 0.05-0.06s with the fastest I've gotten around 0.04s.
    Rendering time for the CBcoil.dae file also showed drastic improvement initially averaging around ~20-30s without BVH. With BVH it takes around 0.03-0.04s. The bench.dae file without BVH
    takes around a minute and averages around 0.1-0.11s to render with BVH enabled. These large reduction in render time can be attributed to several factors as BVH allows for efficient intersection testing by reducing the complexity of intersection tests 
    and inherently helps with parallelism.
</p>
<br>

<h2 align="middle">Part 3: Direct Illumination (20 Points)</h2>
<!-- Walk through both implementations of the direct lighting function.
Show some images rendered with both implementations of the direct lighting function.
Focus on one particular scene with at least one area light and compare the noise levels in soft shadows when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, not uniform hemisphere sampling.
Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis. -->

<h3>
  Walk through both implementations of the direct lighting function.
</h3>
<p>
    I implemented uniform hemisphere sampling by creating num_samples amount of rays that are uniformly spaced in the hemisphere above the hit point (hit_p) and ensuring each ray.min_t = EPS_F. 
    For each ray we check if it intersects with an object and if it does solve the Reflection Equation given in lecture using the Monte Carlo Estimator and add its result to L_out;
    Once we have iterated through all our rays we divide our total radiance (L_out) by the total number of samples/rays (num_samples).
    <br>
    For light importance sampling we iterate through every light source in the scene, and sample it ns_area_light # of times unless it is a delta light in which case we sample it once.
    From the sampling function (sampl_L) we are given a vector (wi) in the world space that points from the hit point (hit_p) to the light source. We want to convert this vector into the object space and check if the z component is negative.
    If the z component is negative it means that the light does not reach the hit point and we can safely ignore its radiance value. If the z-component is positive we want to create a ray given the hit point (hit_p) as its origin and the direction as the created vector (wi).
    Once we create this ray make sure to again set r.min_t = EPS_F, but also set r.max_t = dis_to_light - EPS_F as we only care if there is an object blocking the light and setting this r.max_t allows us to avoid checking the intersection with the light itself.
    With the ray created we can check if there is an intersection, if there is we can also disregard its radiance value as it is blocked by an object. We sum up the radiance of all the rays that do not intersect with an object then divide it by the # of times we sample the light.
    We then add this average to sum of averages of every light in the scene, then we return this sum of averages of every light in the scene then divided by the # of lights in the scene. 
</p>

<h3>
  Show some images rendered with both implementations of the direct lighting function.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <!-- Header -->
    <tr align="center">
      <th>
        <b>Uniform Hemisphere Sampling</b>
      </th>
      <th>
        <b>Light Sampling</b>
      </th>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/p3bunnyh.png" align="middle" width="400px"/>
        <figcaption>CBbunny.dae</figcaption>
      </td>
      <td>
        <img src="images/p3CBbunnyi.png" align="middle" width="400px"/>
        <figcaption>CBbunny.dae</figcaption>
      </td>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/p3spheresh.png" align="middle" width="400px"/>
        <figcaption>CBspheres_lambertian.dae</figcaption>
      </td>
      <td>
        <img src="images/p3spheresi.png" align="middle" width="400px"/>
        <figcaption>CBspheres_lambertian.dae</figcaption>
      </td>
    </tr>
    <br>
  </table>
</div>
<br>

<h3>
  Focus on one particular scene with at least one area light and compare the noise levels in <b>soft shadows</b> when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, <b>not</b> uniform hemisphere sampling.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/p3dragonl1.png" align="middle" width="400px"/>
        <figcaption>1 Light Ray (dragon.dae)</figcaption>
      </td>
      <td>
        <img src="images/p3dragonl4.png" align="middle" width="400px"/>
        <figcaption>4 Light Rays (dragon.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/p3dragonl16.png" align="middle" width="400px"/>
        <figcaption>16 Light Rays (dragon.dae)</figcaption>
      </td>
      <td>
        <img src="images/p3dragonl64.png" align="middle" width="400px"/>
        <figcaption>64 Light Rays (dragon.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<p>
    With the # of samples per pixel being 1 (s = 1) the render will be extremely noisy. This is compounded with the fact that with a low # of samples per area light 
    it is very likely that each point's average radiance is skewed due to random sampling. As we increase l each point will be much more uniform and even with the unpredictable random sampling size of 1,
    the chance that the sample finds the correct radiance is much higher. This is shown as there are clear artifacts for all pictures where l is less than 64
    and in the 64 light rays picture the entire render is very smooth with little visual artifacts.
</p>
<br>

<h3>
  Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis.
</h3>
<p>
  Uniform hemisphere sampling distributes its sampling rays evenly across the hemisphere around the hit point, meaning it treats all directions equally. 
  While this is simple to implement it is often an extremely inefficient way of sampling resulting in slow convergence and increased noise in the rendered images. 
  This effect is exacerbated in complex lighting environments. Importance lighting sampling, on the other hand, focuses its sampling efforts on directions that contribute most to the final image. 
  This is typically done by sampling directions according to the light distribution in the scene. This approach typically leads to faster convergence and reduced noise levels. 
  By concentrating its samples in areas with significant lighting contributions, importance lighting sampling achieves higher quality results with fewer samples in comparison to uniform hemisphere sampling.
</p>
<br>


<h2 align="middle">Part 4: Global Illumination (20 Points)</h2>
<!-- Walk through your implementation of the indirect lighting function.
Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
You will probably want to use the instructional machines for the above renders in order to not burn up your own computer for hours. -->

<h3>
  Walk through your implementation of the indirect lighting function.
</h3>
<p>
    I implemented indirect lighting by recursively calling at_least_one_bounce_radiance where the return was generally one_bounc_radiance + reflection equation where L = at_least_one_bounce_radiance of the next bounce.
    To go more in depth, after being given our initial ray (r) if it still has bounces left (r.depth != 1) I evaluate the bsdf which gives the direction of the bounce (wi), pdf, and f function (reflectance * 1/pi) and using wi I create a new ray (r2). 
    Given this new ray of the next bounce (r2) I check for intersections and solve the reflection equation with the coefficients we solved for previously where radiance (L) is the recursive call to at_least_one_bounce_radiance.
    I add this calcilated radiance to the current ray's (r) one_bounc_radiance and return it. This recursive function stops when r.depth == 1 meaning it is on its last bounce, in which it once again simply returns its (r) one_bounc_radiance.
    <br>
    If isAccumBounces is false, instead of adding the recursive call to the current ray's (r) one_bounc_radiance I add it to an empty Vector3D until r.depth = 1. In which case I proceed as normal.
    This process should result in only returning the radiance of the final bounce.  
    <br>
    To implement Russian Roulette I simply create a double with the termination percent (0.4) and before calculating the next bounce I coinflip with this chance.
    If the coinflip fails, meaning I hit the 40% I simply stop calculating the bounce and return the one_bounc_radiance of the current ray (r).
</p>
<br>

<h3>
  Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/p4bench.png" align="middle" width="400px"/>
        <figcaption>bench.dae</figcaption>
      </td>
      <td>
        <img src="images/p4bunny.png" align="middle" width="400px"/>
        <figcaption>bunny.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3>
  Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/p4spheresd.png" align="middle" width="400px"/>
        <figcaption>Only direct illumination (CBspheres_lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="images/p4spheresid.png" align="middle" width="400px"/>
        <figcaption>Only indirect illumination (CBspheres_lambertian.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
    Indirect illumination is strictly when m > 1 and direct illumination is when m = 0,1. 
    Since indirect illumination is responsible for lighting up spaces where the light does not directly touch its render looks generally brighter as there are no harsh shadows.
    On the other hand direct illumination only lights up the pixels that the light directly interacts with on the first bounce. This means that the surfaces it touches are much brighter than the indirect illumination but the rest of the room is much darker.
</p>
<br>

<h3>
  For CBbunny.dae, render the mth bounce of light with max_ray_depth set to 0, 1, 2, 3, 4, and 5 (the -m flag), and isAccumBounces=false.
  For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, 4, and 5(the -m flag). Use 1024 samples per pixel.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/p4bunnym0.png" align="middle" width="400px"/>
        <figcaption>m = 0 isAccumBounces = false (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/p4abunnym0.png" align="middle" width="400px"/>
        <figcaption>m = 0 isAccumBounces = true (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/p4bunnym1.png" align="middle" width="400px"/>
        <figcaption>m = 1 isAccumBounces = false (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/p4abunnym1.png" align="middle" width="400px"/>
        <figcaption>m = 1 isAccumBounces = true (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/p4bunnym2.png" align="middle" width="400px"/>
        <figcaption>m = 2 isAccumBounces = false (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/p4abunnym2.png" align="middle" width="400px"/>
        <figcaption>m = 2 isAccumBounces = true (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/p4bunnym3.png" align="middle" width="400px"/>
        <figcaption>m = 3 isAccumBounces = false (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/p4abunnym3.png" align="middle" width="400px"/>
        <figcaption>m = 3 isAccumBounces = true (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/p4bunnym4.png" align="middle" width="400px"/>
        <figcaption>m = 4 isAccumBounces = false (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/p4abunnym4.png" align="middle" width="400px"/>
        <figcaption>m = 4 isAccumBounces = true (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/p4bunnym5.png" align="middle" width="400px"/>
        <figcaption>m = 5 isAccumBounces = false (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/p4abunnym5.png" align="middle" width="400px"/>
        <figcaption>m = 5 isAccumBounces = true (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<p>
  When isAccumBounces = false it is showing the illumination that the final bounce contains. This means that when m = 5 and isAccumBounces = false the render is showing the illuminance of the 5th bounce.
  When isAccumBounces = true it is adding up the light from all the bounces up to the max_ray_depth. This means that when m = 4 and isAccumBounces = true the render is the sum of m=0,1,2,3,4 where isAccumBounces = false.
  <br>
  Since light generally loses energy as it travels and interacts with objects, if m increases the rendered image gets correspondingly less bright. The first 2 bounces, however, are still quite strong 
  and this can be seen in the difference between m = 1 and m = 2 when isAccumBounces = true. The addition of the second bounce's illumination which is still quite bright demenstrates that the difference can be quite big when m is small. 
</p>

<h3>
  For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/p4rbunnym0.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 0 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/p4rbunnym1.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 1 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/p4rbunnym2.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 2 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/p4rbunnym3.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 3 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/p4rbunnym4.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 4 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/p4rbunnym100.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 100 (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
    As the max_ray_depth increases the room generally increases in brightness. Past a max_ray_depth of 3 the changes are extremely slight, and this is because as the light bounces around the room 
    it is adding less and less light. This can be seen in the previous section where m=5 and isAccumBounces = false as the room is almost entirely dark. 
    This means the difference between the rendered image of m=4 and m=5 where isAccumBounces = true is the addition of the illumination in the previously described m=5 and isAccumBounces = false picture.
    With the addition of Russian Roulette which terminates the ray based on a random chance every bounce (40%) and the illumination added with each increasing max_ray_depth being smaller every bounce
    the changes past a max_ray_depth of 3 are hardly noticable.
    
</p>
<br>

<h3>
  Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/p4lspheress1.png" align="middle" width="400px"/>
        <figcaption>1 sample per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="images/p4lspheress2.png" align="middle" width="400px"/>
        <figcaption>2 samples per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/p4lspheress4.png" align="middle" width="400px"/>
        <figcaption>4 samples per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="images/p4lspheress8.png" align="middle" width="400px"/>
        <figcaption>8 samples per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/p4lspheress16.png" align="middle" width="400px"/>
        <figcaption>16 samples per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="images/p4lspheress64.png" align="middle" width="400px"/>
        <figcaption>64 samples per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/p4lspheress1024.png" align="middle" width="400px"/>
        <figcaption>1024 samples per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
    As we can see pretty clearly when comparing each of these pictures there is a lot less noise when we increase the samples we take per pixel. 
    In the pictures above, anything under 1024 samples per pixel has quite clear artifacts. This is because each pixel is the average of the accumulated illumination from the sampling rays 
    and with a small amount of sampling rays the chance that average is skewed due to random sampling is extremely high.
</p>
<br>


<h2 align="middle">Part 5: Adaptive Sampling (20 Points)</h2>
<!-- Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
Pick one scene and render it with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth. -->

<h3>
  Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
</h3>
<p>
    The Monte Carlo path tracing is powerful but generates a lot of noise which is usually solved by increasing the samples per pixel. However, arbitrarily increasing the samples per pixel increases computation time and therefore also affects the rendering speed negatively.
    Adaptive sampling is a process used to cut down on this rendering time while still using a high number of samples per pixel. 
    This is possible because some pixels converge faster than others. Adaptive sampling takes advantage of this fact by concentrating the samples in the more complex parts of the image.
    <br>
    How I implemented adapative sampling was to calculate the sum of every sample's illuminance as s1 and the sum of every sample's illuminance squared as s2.
    Using these two values (s1,s2) we can calculate the mean and the variance of all n samples that were taken so far. We use this mean and variance to calculate whether a pixel has converged after n samples were taken using the formula given:
    <p align="middle"><pre align="middle">I = 1.96 * std/sqrt(n)</pre></p>
    We can then check if I is less than or equal to the maxTolerance * mean; if it is, it means that the pixel has converged and we can stop taking samples of it. We don't want to constantly check this as that would increase the computational load of our render 
    so we mod n by the variable samplesPerBatch and when this is equal to 0 (n is divisable by the batch size) we run this check on I. 
</p>
<br>

<h3>
  Pick two scenes and render them with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/dragonp5.png" align="middle" width="400px"/>
        <figcaption>Rendered image (dragon.dae)</figcaption>
      </td>
      <td>
        <img src="images/dragon_ratep5.png" align="middle" width="400px"/>
        <figcaption>Sample rate image (dragon.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/bunnyp5.png" align="middle" width="400px"/>
        <figcaption>Rendered image (bunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/bunny_ratep5.png" align="middle" width="400px"/>
        <figcaption>Sample rate image (bunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>


</body>
</html>
